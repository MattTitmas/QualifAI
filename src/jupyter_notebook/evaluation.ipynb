{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluation\n",
    "Below are a series of graphs used to evaluate the correctness of my dissertation."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Data\\\\testing_CSVs\\\\test_2016_09_18\\\\drivers.txt'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [2], line 12\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mgaussian_process\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkernels\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m RBF, ConstantKernel, WhiteKernel\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mgaussian_process\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m GaussianProcessRegressor\n\u001B[1;32m---> 12\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mModel\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mget_model\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m get_trained_model, convert_to_input, convert_to_training_data\n",
      "File \u001B[1;32m~\\OneDrive - University of Cambridge\\UniWork\\Year3\\Dissertation\\Dissertation\\Model\\__init__.py:1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodel_helper_functions\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m to_categorical, generate_one_hot_helper\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodel_helper_functions\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m generate_one_hot_drivers, generate_one_hot_teams, generate_one_hot_circuits\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mget_model\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m get_trained_model, get_model, convert_to_input, convert_to_training_data\n",
      "File \u001B[1;32m~\\OneDrive - University of Cambridge\\UniWork\\Year3\\Dissertation\\Dissertation\\Model\\model_helper_functions.py:28\u001B[0m\n\u001B[0;32m     25\u001B[0m folder_name \u001B[38;5;241m=\u001B[39m FOLDER\n\u001B[0;32m     26\u001B[0m folder \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mData\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124mtesting_CSVs\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mfolder_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m---> 28\u001B[0m generate_one_hot_driver \u001B[38;5;241m=\u001B[39m partial(generate_one_hot_helper, \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mfolder\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43mdrivers.txt\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mread()\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m))\n\u001B[0;32m     29\u001B[0m generate_one_hot_team \u001B[38;5;241m=\u001B[39m partial(generate_one_hot_helper, \u001B[38;5;28mopen\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfolder\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124mteams.txt\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mread()\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m))\n\u001B[0;32m     30\u001B[0m generate_one_hot_circuit \u001B[38;5;241m=\u001B[39m partial(generate_one_hot_helper, \u001B[38;5;28mopen\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfolder\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124mcircuits.txt\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mread()\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m))\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'Data\\\\testing_CSVs\\\\test_2016_09_18\\\\drivers.txt'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import random\n",
    "from os import makedirs, path, remove\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "\n",
    "\n",
    "from ModelUsage.get_model import get_trained_model, convert_to_input, convert_to_training_data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_location = None\n",
    "store_location = 'Data\\\\Model'\n",
    "csv_location = 'Data\\\\testing_CSVs\\\\First52020.csv'\n",
    "\n",
    "event_date = '09_13'\n",
    "qualifying_session = 'Q2'\n",
    "\n",
    "if qualifying_session == 'Q1':\n",
    "    train_to = 453 # Q1\n",
    "    predict_from = 454 # Q1\n",
    "else:\n",
    "    train_to = 484 # Q2\n",
    "    predict_from = 485 # Q2\n",
    "\n",
    "\n",
    "data = pd.read_csv(csv_location)\n",
    "data = data[0:train_to]\n",
    "\n",
    "data.to_csv('Data\\\\temp.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv('Data\\\\temp.csv')\n",
    "indices = dataframe.index[(dataframe['Date'] == event_date)\n",
    " & (dataframe['SessionName'] == qualifying_session)].tolist()\n",
    "\n",
    "X_kernel = random.sample(indices, len(indices)//2)\n",
    "\n",
    "dataframe, X_kernel = dataframe.drop(index=X_kernel), dataframe.loc[X_kernel]\n",
    "\n",
    "\n",
    "X_train_kernel, y_train_kernel = convert_to_training_data(X_kernel)\n",
    "X_train, y_train = convert_to_training_data(dataframe)\n",
    "if model_location is not None:\n",
    "    # Model folder supplied\n",
    "    model = pickle.load(open(f'{model_location}\\\\model.pkl', 'rb'))\n",
    "    data = pd.read_csv(f'{model_location}\\\\normalisation_constants.csv', index_col=0)\n",
    "    normalisation_constants = data.to_dict(orient='index')\n",
    "else:\n",
    "    # Model not supplied, train a model on the given CSV file\n",
    "    kernel = RBF(length_scale=1)\n",
    "    model, normalisation_constants = get_trained_model(kernel, 'Data\\\\temp.csv', return_constants=True)\n",
    "remove('Data\\\\temp.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if store_location is not None:\n",
    "    # Store folder supplied\n",
    "    if not path.exists(store_location):\n",
    "        makedirs(store_location)\n",
    "    pickle.dump(model, open(f'{store_location}\\\\model.pkl', 'wb'))\n",
    "    constant_dataframe = pd.DataFrame.from_dict(normalisation_constants, orient='index')\n",
    "    constant_dataframe.to_csv(f'{store_location}\\\\normalisation_constants.csv', index=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.kernel_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = pd.read_csv(csv_location)\n",
    "data = data[(data['Date'] == event_date) & (data['SessionName'] == qualifying_session)]\n",
    "\n",
    "grouped_by_driver = data.groupby('DriverName')\n",
    "driver_fastest_laps = grouped_by_driver['LapTime'].min().sort_values()\n",
    "\n",
    "\n",
    "\n",
    "drivers_through = 15 if qualifying_session == 'Q1' else \\\n",
    "    (10 if qualifying_session == 'Q2' else 5)\n",
    "\n",
    "cut_off_time = np.sort(driver_fastest_laps.to_numpy())[drivers_through]\n",
    "order = list(zip(driver_fastest_laps, driver_fastest_laps.index))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "prediction = data.loc[predict_from - 1:]\n",
    "\n",
    "input_to_model = convert_to_input(prediction, normalisation_constants)\n",
    "\n",
    "output = (prediction['LapTime'] - prediction['ExpectedTime']).to_numpy()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(model.predict(input_to_model))\n",
    "print(output)\n",
    "\n",
    "print(model.predict(input_to_model) - output)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from Model.train_hyperparameters import mean_squared_error\n",
    "\n",
    "print(mean_squared_error(model, input_to_model, output))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from Model.train_hyperparameters import random_search as train_hyperparameters\n",
    "kernel = ConstantKernel() * RBF() + RBF()\n",
    "new_kernel, loss = train_hyperparameters(GaussianProcessRegressor, kernel, (1e-10, 1e10),\n",
    "                      X_train, y_train, X_train_kernel, y_train_kernel, size=3, loops=10)\n",
    "\n",
    "print(new_kernel)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trained_model = GaussianProcessRegressor(kernel=new_kernel, optimizer=None)\n",
    "trained_model.fit(X_train, y_train)\n",
    "\n",
    "print(prediction)\n",
    "test = pd.concat([prediction[:1]] * 70)\n",
    "\n",
    "test['LapsCompleted'] = np.arange(70)\n",
    "\n",
    "vals = trained_model.predict(convert_to_input(test, normalisation_constants))\n",
    "\n",
    "plt.plot(np.arange(70), vals, linestyle='dotted')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mean, std = trained_model.predict(input_to_model, return_std=True)\n",
    "\n",
    "print(np.abs(trained_model.predict(input_to_model) - output))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(mean_squared_error(trained_model, input_to_model, output))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(order)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "driver_name = 'Lewis Hamilton'\n",
    "\n",
    "driver_position = 1\n",
    "for pos, t in enumerate(order, start=1):\n",
    "    if t[1] == driver_name:\n",
    "        driver_position = pos\n",
    "        break\n",
    "\n",
    "driver_position"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "drivers_to_knock_out = (drivers_through - driver_position) + 1\n",
    "starting_lap = max(dataframe.iloc[-1]['LapsCompleted'], X_kernel.max(axis='rows')['LapsCompleted']) + 1\n",
    "print(starting_lap)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "drivers_to_check = pd.DataFrame()\n",
    "expected_time = pd.DataFrame()\n",
    "\n",
    "for driver in driver_fastest_laps.index.values:\n",
    "    drivers_to_check = pd.concat((drivers_to_check, data[data['DriverName'] == driver].iloc[:1]))\n",
    "    expected_time = pd.concat((expected_time, data[data['DriverName'] == driver].iloc[:1]['ExpectedTime']))\n",
    "expected_time = expected_time.reset_index(drop=True)\n",
    "drivers_to_check = drivers_to_check.reset_index(drop=True)\n",
    "drivers_to_check['TyreCompound'] = 'Soft'\n",
    "drivers_to_check['TyreUsage'] = 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Without changing laps"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "check = drivers_to_check.copy()\n",
    "check['LapsCompleted'] = starting_lap\n",
    "means, stds = trained_model.predict(convert_to_input(check, normalisation_constants), return_std=True)\n",
    "\n",
    "means = means + expected_time.to_numpy().squeeze()\n",
    "values = None\n",
    "samples = 10000\n",
    "for mean, std in zip(means, stds):\n",
    "    if values is None:\n",
    "        values = np.random.normal(mean, std, samples)\n",
    "    else:\n",
    "        values = np.vstack((values, np.random.normal(mean, std, samples)))\n",
    "\n",
    "times_sorted = np.sort(np.reshape(values, (drivers_through + 5, -1)), axis=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(cut_off_time)\n",
    "mean = np.mean(times_sorted[drivers_through - 1])\n",
    "std = np.std(times_sorted[drivers_through - 1])\n",
    "\n",
    "confidence_interval = mean - 1.96*std, mean + 1.96*std\n",
    "\n",
    "np.mean(times_sorted[drivers_through - 1]) - cut_off_time\n",
    "\n",
    "print(mean)\n",
    "print(mean - cut_off_time)\n",
    "print(confidence_interval[0] - cut_off_time, confidence_interval[1] - cut_off_time)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "repeat_factor = 5000\n",
    "laps = np.arange(drivers_through + 5, dtype=int)\n",
    "total_laps = np.tile(laps, repeat_factor).reshape(repeat_factor, -1)\n",
    "\n",
    "rng = np.random.default_rng()\n",
    "permuted_laps = rng.permuted(total_laps, axis=1) + starting_lap"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "duplicated_data = pd.concat([drivers_to_check] * repeat_factor)\n",
    "duplicated_data['LapsCompleted'] = permuted_laps.reshape(-1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "convert_to_input(duplicated_data, normalisation_constants)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predictions = trained_model.predict(convert_to_input(duplicated_data, normalisation_constants))\n",
    "expected_times = np.tile(expected_time.to_numpy().squeeze(), repeat_factor)\n",
    "predicted_time = expected_times + predictions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "times_sorted = np.sort(np.reshape(predicted_time, (drivers_through + 5, -1)), axis=0)\n",
    "times_sorted"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(cut_off_time)\n",
    "mean = np.mean(times_sorted[drivers_through - 1])\n",
    "std = np.std(times_sorted[drivers_through - 1])\n",
    "\n",
    "confidence_interval = mean - 1.96*std, mean + 1.96*std\n",
    "\n",
    "np.mean(times_sorted[drivers_through - 1]) - cut_off_time\n",
    "\n",
    "print(mean)\n",
    "print(mean - cut_off_time)\n",
    "print(confidence_interval[0] - cut_off_time, confidence_interval[1] - cut_off_time)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Faster version"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "check = drivers_to_check.copy()\n",
    "check['LapsCompleted'] = starting_lap\n",
    "check = pd.concat([check] * (drivers_through + 5)).reset_index()\n",
    "check['LapsCompleted'] += laps.repeat(drivers_through + 5)\n",
    "\n",
    "\n",
    "model_input = convert_to_input(check, normalisation_constants)\n",
    "means, stds = trained_model.predict(convert_to_input(check, normalisation_constants), return_std=True)\n",
    "\n",
    "driver_probs = dict()\n",
    "for count, driver in enumerate(check['DriverName'].unique()):\n",
    "    indices = check[check['DriverName'] == driver].index.tolist()\n",
    "    predictions_for_driver = expected_time.to_numpy().squeeze()[count] + means[indices]\n",
    "    driver_probs[driver] = {lap: (predictions_for_driver[lap], stds[indices][lap]) for lap in range(len(predictions_for_driver))}\n",
    "\n",
    "cutoff_times = np.array([])\n",
    "samples = 100\n",
    "orders = 1000\n",
    "for order in range(orders):\n",
    "    random.shuffle(laps)\n",
    "    values = None\n",
    "    for count, driver in enumerate(check['DriverName'].unique()):\n",
    "        mean, std = driver_probs[driver][laps[count]]\n",
    "        if values is None:\n",
    "            values = np.random.normal(mean, std, samples)\n",
    "        else:\n",
    "            values = np.vstack((values, np.random.normal(mean, std, samples)))\n",
    "    times_sorted = np.sort(values, axis=0)\n",
    "    cutoff_times = np.concatenate([cutoff_times, times_sorted[drivers_through-1]])\n",
    "\n",
    "mean = np.mean(cutoff_times)\n",
    "std = np.std(cutoff_times)\n",
    "\n",
    "confidence_interval = mean - 1.96*std, mean + 1.96*std\n",
    "print(mean, cut_off_time)\n",
    "print(mean - cut_off_time)\n",
    "print(confidence_interval[0] - cut_off_time, confidence_interval[1] - cut_off_time)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
